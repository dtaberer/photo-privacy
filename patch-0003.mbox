From 9aa7b6de0a0e4c2a1c66cccccccccccccccccccc Mon Sep 17 00:00:00 2001
From: Code Copilot <code-copilot@noreply.local>
Date: Wed, 20 Aug 2025 12:10:00 -0400
Subject: [PATCH] scrubber: OCR downscale knob + README (Local vs CDN,
 troubleshooting)

---
 README.md                                     |  77 ++++++++++++++++++++++++++
 src/features/privacy/PhotoPrivacyScrubber.tsx | 103 +++++++++++++++++++++++---
 2 files changed, 169 insertions(+), 11 deletions(-)

Diff: README.md
--- /dev/null
+++ b/README.md
@@ -0,0 +1,77 @@
+# Photo Privacy – Notes
+
+## Models: Local vs CDN
+
+The scrubber tries to load face-api models from **Local** first (served from `public/models/`). If not found, it falls back to **CDN** (jsDelivr, pinned version).
+
+### Make it say "Local"
+1. Ensure these files exist:
+   - `public/models/tiny_face_detector_model-weights_manifest.json`
+   - `public/models/tiny_face_detector_model.bin`
+2. Use the helper:
+   ```bash
+   npm run models:fetch
+   ```
+3. In the UI, click **Re-check models**. The badge shows `Models: Local` when both files resolve locally.
+
+### Verify manually
+- Open `http://localhost:<port>/<base>/models/tiny_face_detector_model.bin` in the browser. It should return the file (200), not 404.
+- In DevTools → Network, the `HEAD` requests to `/models/...` should be 200.
+
+## OCR downscale for speed
+
+Large photos slow OCR. You can set **OCR max size** in the UI. When set to a positive value, the app downsizes the image (max dimension) before OCR while keeping full resolution for redactions and final export.
+
+- Set to `0` to disable downscale.
+- Default: `1600`.
+
+## Troubleshooting
+
+**Models show CDN**
+- Ensure files are in `public/models/` (not `src/`).
+- If deployed under a subpath, verify Vite `base` matches. The app uses `import.meta.env.BASE_URL` for checks.
+
+**Model 404s**
+- Run `npm run models:fetch`.
+- Confirm the files exist on disk and are committed or present in your deploy artifact.
+
+**Worker not used / OCR blocks UI**
+- Some environments lack `createImageBitmap` or `module` workers. The app falls back to main-thread OCR automatically.
+
+**EXIF orientation looks wrong**
+- Some images lack orientation tags. The scrubber draws as-is when EXIF is missing.
+
+**Running in WSL**
+- Ensure the project path and browser can access `http://localhost` across the boundary. If not, try the Windows-side browser or `wslview`.
+
+---
+
+## Scripts
+
+```bash
+npm run models:fetch  # downloads face-api TinyFaceDetector models into public/models
+```
+
+## Files
+
+- `scripts/fetch-face-models.mjs` – postinstall helper to fetch models.
+- `src/workers/ocr.worker.ts` – Tesseract OCR offloaded to a Web Worker (with fallback).
+- `src/features/privacy/PhotoPrivacyScrubber.tsx` – main UI/component.

Diff: src/features/privacy/PhotoPrivacyScrubber.tsx
--- a/src/features/privacy/PhotoPrivacyScrubber.tsx
+++ b/src/features/privacy/PhotoPrivacyScrubber.tsx
@@ -8,11 +8,12 @@ type RedactionMode = "fill" | "blur" | "pixelate";
 type DetectedBox = { x: number; y: number; w: number; h: number; score?: number; label?: string };
-
-type ProcessOptions = { mode: RedactionMode; blurRadius: number; pixelSize: number; textConfidence: number; faceScore: number };
+
+type ProcessOptions = { mode: RedactionMode; blurRadius: number; pixelSize: number; textConfidence: number; faceScore: number; ocrMaxDim: number };
 
-const DEFAULTS: ProcessOptions = { mode: "fill", blurRadius: 18, pixelSize: 18, textConfidence: 0.6, faceScore: 0.5 };
+const DEFAULTS: ProcessOptions = { mode: "fill", blurRadius: 18, pixelSize: 18, textConfidence: 0.6, faceScore: 0.5, ocrMaxDim: 1600 };
@@
 async function loadFaceModels() { await ensureDeps(); const root = await resolveFaceModelRoot(); await faceapi.nets.tinyFaceDetector.loadFromUri(root); }
@@
 async function detectFacesOnCanvas(canvas: HTMLCanvasElement, minScore: number): Promise<DetectedBox[]> { await loadFaceModels(); const options = new faceapi.TinyFaceDetectorOptions({ inputSize: 416, scoreThreshold: clamp(minScore, 0.1, 0.9) }); const results = await faceapi.detectAllFaces(canvas, options); return results.map((r: any) => ({ x: r.box.x, y: r.box.y, w: r.box.width, h: r.box.height, score: r.score, label: "face" })); }
+
+function scaledCanvas(src: HTMLCanvasElement, maxDim: number): HTMLCanvasElement {
+  const w = src.width; const h = src.height; const max = Math.max(w, h);
+  if (!maxDim || maxDim <= 0 || max <= maxDim) return src;
+  const ratio = maxDim / max;
+  const out = document.createElement("canvas");
+  out.width = Math.max(1, Math.round(w * ratio));
+  out.height = Math.max(1, Math.round(h * ratio));
+  const ctx = out.getContext("2d", { willReadFrequently: true });
+  if (!ctx) return src;
+  ctx.drawImage(src, 0, 0, out.width, out.height);
+  return out;
+}
@@
-  const detectTextViaWorker = useCallback(async (canvas: HTMLCanvasElement, threshold01: number): Promise<DetectedBox[]> => {
+  const detectTextViaWorker = useCallback(async (canvas: HTMLCanvasElement, threshold01: number, ocrMaxDim: number): Promise<DetectedBox[]> => {
     const worker = ocrWorkerRef.current;
-    if (!worker || typeof createImageBitmap !== "function") {
+    const src = scaledCanvas(canvas, Math.floor(ocrMaxDim || 0));
+    if (!worker || typeof createImageBitmap !== "function") {
       // @ts-ignore
       const Tesseract = (await import("tesseract.js")).default || (await import("tesseract.js"));
-      const { data } = await Tesseract.recognize(canvas, "eng", { logger: () => {} });
+      const { data } = await Tesseract.recognize(src, "eng", { logger: () => {} });
       const cutoff = clamp(threshold01, 0, 1) * 100;
       return (data.words || []).filter((w: any) => w.confidence >= cutoff).map((w: any) => ({ x: w.bbox.x0, y: w.bbox.y0, w: w.bbox.x1 - w.bbox.x0, h: w.bbox.y1 - w.bbox.y0, score: w.confidence / 100, label: "text" }));
     }
-    const bitmap = await createImageBitmap(canvas);
+    const bitmap = await createImageBitmap(src);
     return new Promise<DetectedBox[]>((resolve, reject) => {
       const onMsg = (ev: MessageEvent) => {
         const d: any = ev.data || {};
         worker.removeEventListener("message", onMsg);
         if (d.ok) resolve(d.words as DetectedBox[]);
         else reject(new Error(d.error || "ocr failed"));
       };
       worker.addEventListener("message", onMsg);
-      worker.postMessage({ bitmap, lang: "eng", confidence: threshold01 }, [bitmap as any]);
+      worker.postMessage({ bitmap, lang: "eng", confidence: threshold01 }, [bitmap as any]);
     });
   }, []);
@@
-    setStatus("Detecting text… (non-blocking)");
-    let wordsFound: DetectedBox[] = [];
-    try { wordsFound = await detectTextViaWorker(canvas, opts.textConfidence); } catch {}
+    setStatus("Detecting text… (non-blocking)");
+    let wordsFound: DetectedBox[] = [];
+    try { wordsFound = await detectTextViaWorker(canvas, opts.textConfidence, opts.ocrMaxDim); } catch {}
@@
           <div className="border rounded-2xl p-3 space-y-3">
             <h2 className="font-medium">Redaction Options</h2>
             <div className="flex flex-wrap gap-3 items-center">
@@
             </div>
             <div className="flex flex-wrap gap-3 items-center">
               <label className="flex items-center gap-2 text-sm">
                 <span>Face score ≥</span>
                 <input type="number" step={0.05} min={0.1} max={0.9} value={opts.faceScore} onChange={(e) => setOpts((o) => ({ ...o, faceScore: Number(e.target.value) }))} className="w-20 border rounded px-2 py-1" />
               </label>
+              <label className="flex items-center gap-2 text-sm">
+                <span>OCR max size</span>
+                <input type="number" min={0} max={4000} value={opts.ocrMaxDim} onChange={(e) => setOpts((o) => ({ ...o, ocrMaxDim: Math.max(0, Number(e.target.value)) }))} className="w-24 border rounded px-2 py-1" />
+                <span className="text-xs text-gray-500">px (0=off)</span>
+              </label>
             </div>
           </div>
@@
         </aside>
       </div>
     </div>
   );
 }
-- 
2.46.0
